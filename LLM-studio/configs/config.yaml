# LLM Training Configuration

# Model Configuration
model:
  path: ""  # Model path
  use_lora: true  # Set to "true" for LoRA fine-tuning, "false" for full-parameter fine-tuning (default: LoRA)
  # LoRA-specific parameters (only used when use_lora is true)
  lora_target_modules: 
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.05

# Training Configuration
training:
  output_dir: "output"  # Output directory
  batch_size: 1  # Batch size per device
  gradient_accumulation_steps: 16  # Gradient accumulation steps
  learning_rate: 1e-4
  warmup_steps: 50
  max_grad_norm: 1.0
  epochs: 10  # Number of training epochs
  logging_steps: 5
  save_steps: 10
  save_total_limit: 3  # Number of checkpoints to keep
  save_on_each_node: true
  gradient_checkpointing: true  # Enable gradient checkpointing
  bf16: true  # Use bfloat16 precision
  fp16: false  # Use float16 precision
  remove_unused_columns: false
  optimizer: "adamw_torch"  # Optimizer
  dataloader_pin_memory: true
  group_by_length: true
  lr_scheduler_type: "cosine"  # Learning rate scheduler type
  weight_decay: 0.01
  max_steps: -1  # -1 means use epochs instead
  save_strategy: "steps"
  save_safetensors: true
  overwrite_output_dir: true

# Data Configuration
data:
  path: "./data/train.json"  # Training data path
  max_length: 4048  # Maximum sequence length
